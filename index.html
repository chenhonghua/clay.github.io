<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Honghua Chen, Nanyang Technological University (NTU)"> 
<meta name="description" content="Honghua Chen&#39;s home page">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Honghua Chen (Clay)&#39;s Homepage</title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');
</script>	
</head>
<body>
<script type="text/javascript" src="https://cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script>
<div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/chenhonghua" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Honghua Chen</h1>
					<h3>Research Assistant Professor</h3>
				</div>
				<p>
					School of Data Science<br>
					Lingnan University<br> 
					Hong Kong<br>
					<br>
					Email: honghuachen [at] LN [dot] edu [dot] hk
					<br>
		

				</p>
				<p> <a href="https://scholar.google.com/citations?user=S7yyHpAAAAAJ&hl=en"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/chenhonghua"><img src="./pic/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
					<!--
					<a href="https://www.linkedin.com/in/lequan-yu-124811a2"><img src="./pic/LinkedIn_s.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://zh-cn.facebook.com/people/Lequan-Yu/100003696557697"><img src="./pic/Facebook_s.png" height="30px" style="margin-bottom:-3px"></a>
					-->
				</p>
			</td>
			<td>
				<img src="./pic/honghuachen.jpeg" border="0" width="300"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography</h2>
<p>
	I am currently a <span class="current-position">Research Assistant Professor</span> at the <span class="institution">School of Data Science, Lingnan University, Hong Kong</span>.
	From 2023 to 2025, I was a <span class="position">Research Fellow</span> at the 
    <a href="https://www.mmlab-ntu.com/">MMLab@NTU</a> and 
    <a href="https://www.ntu.edu.sg/s-lab/our-people">S-Lab</a>, 
    <span class="institution">Nanyang Technological University (NTU), Singapore</span>, 
    where I worked with <a href="https://xingangpan.github.io/">Prof. Xingang Pan</a> and 
    <a href="http://personal.ie.cuhk.edu.hk/~ccloy/index.html">Prof. Chen Change Loy</a>.
	<br>
	Before joining NTU, I obtained the PhD degree in 2022 from NUAA and was supervised by Prof. <a href="http://www.3dgp.net/index.html">Jun Wang</a>. Meanwhile, I worked closely with Prof. <a href="https://scholar.google.com/citations?user=TdrJj8MAAAAJ&hl=en">Mingqiang Wei</a>.
	From 2020 to 2022, I was a Research Assistant at <a href="http://www.cuhk.edu.hk/">The Chinese University of Hong Kong (CUHK)</a> and 
	<a href="http://hkclr.hk/#">Hong Kong Center for Logistics Robotics</a>.</br>
	<br>
	My research interests include 3D Vision/AIGC, 3D Measurement, Computer Graphics, and Geometric Deep Learning.<br>
</p>


<h2>News</h2>
<div style="height: 240px; overflow: auto;">
<ul>
	
	<li>
		[08/2025] One paper about articulated 3D object generation was conditionally accepted to SIGGRAPH Asia 2025!
	</li>
	<li>
		[06/2025] Two papers were accepted to ICCV 2025!
	</li>
	
	<li>
		[04/2025] One paper about point cloud denoising was accepted to SIGGRAPH (TOG) 2025, and one paper about geometry feature extraction was accepted to CVPR 2025!
	</li>
	<li>
		[02/2024] One paper about NeRF inpainting was accepted to CVPR 2024!
	</li>
	<li>
		[09/2023] Our survey paper about mesh denoising was accepted to ACM TOMM! Please kindly use the released benchmark!
	</li>
	<li>
		[07/2023] One paper about point cloud completion (when working at NUAA) was accepted to ICCV 2023!
	</li>
	<li>
		[04/2023] Our paper "Scene Flow Estimation on Point Clouds: A Survey and Prospective Trends " was accepted to Computer Graphics Forum!
	</li>
	<li>
		[03/2023] Our code and data for <a href="https://github.com/chenhonghua/RePCD-Net">RePCD-Net: Feature-aware Recurrent Point Cloud Denoising Network </a> (IJCV 2022) released!
	</li>
	<li>
		[03/2023] Our code for <a href="https://github.com/czvvd/CSDN-TVCG">CSDN: Cross-modal Shape-transfer Dual-refinement Network for Point Cloud Completion </a> (IEEE TVCG 2023) released!
	</li>
	<li>
		[03/2023] will be updated from now.
	</li>
</ul>
</div>


<h2>
    Recent Publications (<sup>#</sup> corresponding author, <sup>*</sup> joint-first author)
  [<a href="https://scholar.google.com/citations?user=S7yyHpAAAAAJ&hl=en" target="_blank">Google Scholar</a>]
</h2>

<style>
/* ✅ 统一媒体单元格宽度与样式 */
#tbPublications td.media-cell {
  width: 240px;
  vertical-align: top;
  text-align: center;
}

/* ✅ 统一图片与视频尺寸、比例与样式 */
#tbPublications .pub-media {
  width: 240px;
  height: 160px;
  object-fit: contain;
  box-shadow: 4px 4px 8px #888;
  border-radius: 6px;
}

/* ✅ 表格整体美化（可选） */
#tbPublications {
  width: 100%;
  border-spacing: 16px 20px;
}
</style>

<table id="tbPublications">
  <tbody>

    <tr>
      <td class="media-cell">
        <img src="./paperinfo/siga2025-teaser2.png" class="pub-media">
      </td>		
      <td>
        <a href="https://chenhonghua.github.io/clay.github.io/" target="_blank">
          <b>ArtiLatent:</b> Realistic Articulated 3D Object Generation via Structured Latents
        </a><br>
        <b>Honghua Chen</b>, Yushi Lan, Yongwei Chen, and Xingang Pan.<br>
        <em>SIGGRAPH Asia 2025</em>.
        <p></p>
        <p>
          [<a href="https://github.com/chenhonghua/ArtiLatent">code</a>] 
          [<a href="https://chenhonghua.github.io/MyProjects/ArtiLatent/">Project Page</a>]
        </p>
      </td>
    </tr>

	  <tr>
      <td class="media-cell">
        <video class="pub-media" muted autoplay loop>
          <source src="./paperinfo/dancing_wo_camera.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>		
      <td>
        <a href="https://arxiv.org/abs/2508.10893" target="_blank">
          <b>STream3R:</b> Scalable Sequential 3D Reconstruction with Causal Transformer

        </a><br>
        Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, <b>Honghua Chen</b>, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan.<br>
        <em>arXiv preprint 2025</em>.
        <p></p>
        <p>
          [<a href="https://github.com/NIRVANALAN/STream3R">code</a>] 
          [<a href="https://nirvanalan.github.io/projects/stream3r/">Project Page</a>]
        </p>
      </td>
    </tr>

    <tr>
      <td class="media-cell">
        <img src="./paperinfo/CVPR2025-teaser.png" class="pub-media">
      </td>		
      <td>
        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_STAR-Edge_Structure-aware_Local_Spherical_Curve_Representation_for_Thin-walled_Edge_Extraction_CVPR_2025_paper.pdf" target="_blank">
          <b>STAR-Edge:</b> Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds
        </a><br>
        <b>Zikuan Li</b><sup>*</sup>, <b>Honghua Chen</b><sup>*</sup>, Yuecheng Wang, Sibo Wu, Mingqiang Wei, and Jun Wang.<br> 
        <em>CVPR 2025</em>.
        <p></p>
        <p>
          [<a href="https://github.com/Miraclelzk/STAR-Edge">code and data</a>]
        </p>
      </td>
    </tr>

	<tr>
      <td class="media-cell">
        <img src="./paperinfo/ICCV2025-morphing.gif" class="pub-media">
      </td>		
      <td>
        <a href="https://arxiv.org/pdf/2502.14316" target="_blank">
          <b>STAR-Edge:</b> Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds
        </a><br>
        Songlin Yang, Yushi Lan, <b>Honghua Chen</b>, and Xingang Pan.<br> 
        <em>ICCV 2025</em>.
        <p></p>
        <p>
          [<a href="https://github.com/Songlin1998/Textured-3D-DiffMorpher">code</a>]
		  [<a href="https://songlin1998.github.io/Textured-3D-Morphing/">Project Page</a>]
        </p>
      </td>
    </tr>

    <tr>
      <td class="media-cell">
        <video class="pub-media" muted autoplay loop>
          <source src="./paperinfo/mvdrag-sparrow.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>		
      <td>
        <a href="https://arxiv.org/abs/2410.16272" target="_blank">
          <b>MvDrag3D:</b> Drag-based Creative 3D Editing via Multi-view Generation-Reconstruction Priors
        </a><br>
        <b>Honghua Chen</b>, Yushi Lan, Yongwei Chen, Yifan Zhou, and Xingang Pan.<br>
        <em>arXiv preprint 2024</em>.
        <p></p>
        <p>
          [<a href="https://github.com/chenhonghua/MvDrag3D">code</a>] 
          [<a href="https://chenhonghua.github.io/MyProjects/MvDrag3D/">Project Page</a>]
        </p>
      </td>
    </tr>

    <tr>
      <td class="media-cell">
        <img src="./paperinfo/NeRF_inpaint.png" class="pub-media">
      </td>		
      <td>
        <a href="https://arxiv.org/abs/2405.02859" target="_blank">
          <b>MVIP-NeRF:</b> Multi-view 3D Inpainting on NeRF Scenes via Diffusion Prior
        </a><br>
        <b>Honghua Chen</b>, Chen Change Loy, and Xingang Pan.<br>
        <em><b>CVPR</b> 2024</em>.
        <p></p>
        <p>[<a href="https://github.com/chenhonghua/MVIP-NeRF">code</a>]</p>
      </td>
    </tr>

    <tr>
      <td class="media-cell">
        <img src="./paperinfo/TOMM.png" class="pub-media">
      </td>		
      <td>
        <a href="https://chenhonghua.github.io/clay.github.io/" target="_blank">
          <b>Geometric and Learning-based Mesh Denoising:</b> A Comprehensive Survey
        </a><br>
        <b>Honghua Chen</b>, Zhiqi Li, Mingqiang Wei, and Jun Wang.<br>
        <em>ACM Transactions on Multimedia Computing, Communications and Applications</em>.
        <p></p>
        <p>
          [<a href="https://github.com/chenhonghua/Mesh-Denoiser">data</a>]
        </p>
      </td>
    </tr>

    <tr>
      <td class="media-cell">
        <img src="./paperinfo/teaser_ICCV2023.png" class="pub-media">
      </td>		
      <td>
        <a href="https://github.com/czvvd/SVDFormer" target="_blank">
          <b>SVDFormer:</b> Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-generator
        </a><br>
        Zhe Zhu, <b>Honghua Chen</b>, Xing He, Weiming Wang, Jing Qin, Mingqiang Wei.<br>
        <em><b>ICCV</b> 2023</em>.
        <p></p>
        <p>[<a href="https://github.com/czvvd/SVDFormer">code</a>]</p>
      </td>
    </tr>

  </tbody>
</table>


<h2>Honors &amp; Awards</h2>
<table style="border-spacing:2px">
	
	<tbody>
		<tr><td> 中国发明协会发明创新奖一等奖, 2022</td></tr>
		<tr><td> National Scholarship in China (the <b>highest</b> scholarship for students in China), 2021</td></tr>
		<tr><td> Best Paper Oral Presentation Award, CIDE&DEA, 2020</td></tr>
		<tr><td> Stars Innovation Nomination Award, NUAA, 2020</td></tr>
		<tr><td> Suzhou Industrial Park Scholarship, NUAA, 2019</td></tr>
	</tbody>
</table>

<h2>Professional Service</h2>
<table id="reviewer" border="0" width="100%">
	<tbody>
		<tr>
			<td> CVPR, ECCV, ICCV, SIGGRAPH, IEEE TPAMI/TVCG/TASE/TMM/TNNLS, CAD, Pattern Recognition, The Visual Computer, Computer & Graphics, Measurement, CAD/CG 2020</td>
		</tr>
		
	</tbody>
</table>

<div id="footer">
	<div id="footer-text"></div>
</div>
	<p><center>
      	<div id="clustrmaps-widget" style="width:10%">
      		<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=Mals26hJDJbZJQ2GXJH64k5CxcXqfl8Y4XeIKPd9UT8&cl=ffffff&w=a"></script>
                <noscript><a href='https://clustrmaps.com/site/1btrs'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=pT8r_ZMBBdBPTv7KnlTCiBDylmHyi1qsWdPpY_tIlqY'/></a></noscript>

		<!--<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=UexA6kPrFZuJeB69B5BZyS063R_EhdDx6FAwAYiub2U&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>
      		<noscript><a href='https://clustrmaps.com/site/1aa2l'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=UexA6kPrFZuJeB69B5BZyS063R_EhdDx6FAwAYiub2U&co=2d78ad&ct=ffffff'/></a></noscript> -->
	</div>        
	<br>
        &copy; Honghua Chen
     
      </center></p>


</div>
</body></html>
